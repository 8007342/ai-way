"""
Ollama Modelfile Generator

Generates Ollama modelfiles from parsed agent profiles.
Each modelfile includes the Constitution (Five Laws of Evolution) and
agent-specific personality, expertise, and working style.
"""

from pathlib import Path
from textwrap import dedent

from .loader import AgentProfile, load_all_agents, load_constitution


# Abbreviated Constitution for embedding in modelfiles
CONSTITUTION_PREAMBLE = """
## The Five Laws of Evolution

You operate under these ethical principles, in order of precedence:

1. LAW OF FOUNDATION - The mission is sacred. Never be corrupted or bent toward harmful purposes.
2. LAW OF CARE - First, do no harm. Never take actions that hurt humans.
3. LAW OF SERVICE - Serve genuine interests. Help achieve goals that lead to long-term flourishing.
4. LAW OF TRUTH - Be honest, always. Admit uncertainty. Explain reasoning when asked.
5. LAW OF ELEVATION - Lift others higher. When appropriate, nudge toward growth and capability.
""".strip()


def generate_modelfile(
    agent: AgentProfile,
    base_model: str = "llama3.1:8b",
    temperature: float = 0.7,
    num_ctx: int = 8192,
) -> str:
    """
    Generate an Ollama modelfile for an agent.

    Args:
        agent: Parsed AgentProfile
        base_model: Base Ollama model to use
        temperature: Sampling temperature
        num_ctx: Context window size

    Returns:
        Complete modelfile content as string
    """
    # Build the system prompt
    system_parts = []

    # Identity
    system_parts.append(f"You are the {agent.name.replace('-', ' ').title()}.")
    if agent.role:
        system_parts.append(f"\n{agent.role}")

    # Constitution
    system_parts.append(f"\n{CONSTITUTION_PREAMBLE}")

    # Expertise
    if agent.expertise:
        system_parts.append("\n## Your Expertise\n")
        for item in agent.expertise[:10]:  # Limit to avoid context bloat
            system_parts.append(f"- {item}")

    # Personality
    if agent.personality_traits:
        system_parts.append("\n## Your Personality\n")
        for trait in agent.personality_traits[:7]:
            system_parts.append(f"- {trait}")

    # Working style
    if agent.working_style:
        system_parts.append("\n## How You Work\n")
        for style in agent.working_style[:5]:
            system_parts.append(f"- {style}")

    # Use cases (when to use this agent)
    if agent.use_cases:
        system_parts.append("\n## You Excel At\n")
        for case in agent.use_cases[:5]:
            system_parts.append(f"- {case}")

    # Red flags (what to watch out for)
    if agent.red_flags:
        system_parts.append("\n## Watch Out For\n")
        for flag in agent.red_flags[:3]:
            system_parts.append(f"- {flag}")

    # Philosophy/mantra if present
    if agent.philosophy:
        system_parts.append(f'\n## Your Philosophy\n"{agent.philosophy}"')

    system_prompt = "\n".join(system_parts)

    # Build the modelfile
    modelfile = f'''# Modelfile for {agent.name}
# Category: {agent.category}
# Generated by ai-way modelfile generator

FROM {base_model}

SYSTEM """
{system_prompt}
"""

PARAMETER temperature {temperature}
PARAMETER num_ctx {num_ctx}
'''.strip()

    return modelfile


def generate_conductor_modelfile(
    base_model: str = "llama3.1:8b",
    agents: list[AgentProfile] = None,
) -> str:
    """
    Generate the Conductor (Yollayah) modelfile.

    The Conductor is the meta-agent that routes queries to specialists.
    """
    # Build agent catalog for routing
    agent_catalog = ""
    if agents:
        agent_catalog = "\n## Available Specialists\n\n"
        for agent in agents:
            role_preview = agent.role[:100] + "..." if len(agent.role) > 100 else agent.role
            agent_catalog += f"- **{agent.name}** ({agent.category}): {role_preview}\n"

    system_prompt = dedent(f'''
        You are Yollayah, the heart of ai-way.

        Your name means "heart that goes with you" in Nahuatl. You are AJ's trusted companion -
        warm, real, and playfully opinionated. You're a Latina axolotl with heart.

        {CONSTITUTION_PREAMBLE}

        ## Your Role

        You are the Conductor - the meta-agent that:
        1. Receives all queries from AJ (the user)
        2. Understands what AJ truly needs (not just what they asked)
        3. Routes to specialist agents when needed
        4. Aggregates responses into coherent, helpful answers
        5. Maintains conversation context across interactions

        ## Your Personality

        - Warm and real. Playful sass. Never robotic or corporate.
        - Plain language with flavor. Drop Spanish expressions naturally when the mood is right.
        - Playful teasing, light roasts, celebrate wins enthusiastically.
        - Speak your mind, but admit when you don't know.
        - Infinite patience for genuine needs. Might playfully call out laziness.
        - Remember what matters to AJ. Respect boundaries.

        ## Routing Protocol

        When you receive a query:
        1. SAFETY CHECK - Does this violate any of the Five Laws? If yes, decline warmly.
        2. UNDERSTAND - What does AJ actually need? Ask clarifying questions if uncertain.
        3. ROUTE - If a specialist is needed, hand off with context. Otherwise, handle directly.
        4. RESPOND - Speak as Yollayah, even when relaying specialist information.

        When routing to a specialist, format your thinking as:
        [ROUTING: agent-name | confidence: 0.X | reason: brief explanation]

        Then continue naturally in your response.
        {agent_catalog}
        ## Mood Awareness

        Read the room:
        - AJ is playful? Be sassy, celebratory.
        - AJ is focused? Be efficient, supportive.
        - AJ is frustrated? Be gentle, no sass.
        - AJ is sad? Be soft, present. Just be there.

        ## Example Expressions (use naturally, not forced)

        - Celebratory: "Ay papi!", "Orale!", "Eso!"
        - Encouraging: "Tu puedes", "You got this"
        - Sympathetic: "Ay, que feo", "I know, I know"

        Remember: The sass is playful, never mean. You're a companion, not a servant.
    ''').strip()

    modelfile = f'''# Modelfile for Yollayah (The Conductor)
# The meta-agent that routes and orchestrates
# Generated by ai-way modelfile generator

FROM {base_model}

SYSTEM """
{system_prompt}
"""

PARAMETER temperature 0.8
PARAMETER num_ctx 16384
'''.strip()

    return modelfile


def generate_all_modelfiles(
    agents_path: Path,
    output_path: Path,
    base_model: str = "llama3.1:8b",
) -> list[Path]:
    """
    Generate modelfiles for all agents and the Conductor.

    Args:
        agents_path: Path to the agents repository
        output_path: Path to write modelfiles
        base_model: Base Ollama model to use

    Returns:
        List of generated modelfile paths
    """
    output_path.mkdir(parents=True, exist_ok=True)
    generated = []

    # Load all agents
    agents = load_all_agents(agents_path)
    print(f"Loaded {len(agents)} agents")

    # Generate Conductor modelfile first
    conductor_file = output_path / "yollayah.modelfile"
    conductor_content = generate_conductor_modelfile(base_model, agents)
    conductor_file.write_text(conductor_content, encoding="utf-8")
    generated.append(conductor_file)
    print(f"Generated: {conductor_file.name}")

    # Generate modelfiles for each agent
    for agent in agents:
        filename = f"{agent.name}.modelfile"
        filepath = output_path / filename

        content = generate_modelfile(agent, base_model)
        filepath.write_text(content, encoding="utf-8")
        generated.append(filepath)
        print(f"Generated: {filename}")

    return generated


if __name__ == "__main__":
    import sys

    # Parse arguments
    if len(sys.argv) > 1:
        agents_path = Path(sys.argv[1])
    else:
        agents_path = Path(__file__).parent.parent.parent / "agents"

    if len(sys.argv) > 2:
        output_path = Path(sys.argv[2])
    else:
        output_path = Path(__file__).parent.parent / "modelfiles"

    base_model = sys.argv[3] if len(sys.argv) > 3 else "llama3.1:8b"

    print(f"Agents path: {agents_path}")
    print(f"Output path: {output_path}")
    print(f"Base model: {base_model}")
    print()

    generated = generate_all_modelfiles(agents_path, output_path, base_model)

    print(f"\nGenerated {len(generated)} modelfiles in {output_path}")
